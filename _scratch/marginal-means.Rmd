---
title: "Untitled"
author: "Tristan Mahr"
date: "2024-05-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```








```{r}
library(tidyverse)
d <- rtdists::speed_acc |> 
  tibble::as_tibble() |> 
  mutate(
    correct = as.character(response) == as.character(stim_cat),
    correct = as.numeric(correct),
    freq = frequency |> 
      as.character() |> 
      stringr::str_replace("nw_", "")
  )
d
a <- d |> 
  filter(!censor) |> 
  group_by(id, condition, stim_cat, frequency) |> 
  summarise(
    n_correct = sum(correct),
    n_trials = n(),
    n_incorrect = sum(!correct)
  ) |> 
  ungroup()

library(lme4)
m <- glmer(
  cbind(n_correct, n_trials - n_correct) ~ condition + (condition | id),
  family = binomial(),
  data = a
)
summary(m)
ggplot(a) + 
  aes(x = condition, y = n_correct / n_trials) + 
  # geom_point() +
  # geom_line(aes(group = id)) +
  stat_summary(aes(group = id)) +
  stat_summary(aes(group = id), geom = "line")

lattice::dotplot(ranef(m))

d |> 
  distinct(stim_cat, frequency)
d |> 
  distinct(stim, stim_cat, frequency)

d$stim_cat
d$response
d
ggplot(d) + aes(x = id, y = correct, color = condition) + stat_summary() + facet_wrap(~stim_cat)

```

In this *lexical decision* experiment, participants were given a string
of letters onscreen and had to indicate whether the string was a word or a
nonword. Half the time the string was a word and half the time the
string was a nonword, as recorded in the `stimcat` variable.

```{r}
d |> 
  count(stim_cat)
```

Seventeen participants each completed 20 blocks of trials. In each block, they were
instructed to aim for *accuracy* or *speed* (`condition`).

```{r}
d |> 
  count(condition, stim_cat)
```

The real words varied in frequency (`freq`: high, low and very low), and the
nonwords were created by taking real words and randomly replacing the
free vowels (i.e., any vowel except for U in QU) with other vowels.

```{r}
d |> 
  count(condition, stim_cat, freq)
```

So, everything is neatly balanced by design but some bad trials had to be
discarded.


## The baseline model

Let's consider the average accuracy for real words on the *speed* ðŸƒâ€â™€ï¸ trials. We fit a mixed effects logistic regression model. 

The observed data is the number of successes $y_i$ in $n_i$ trials for each participant $i$. We want to estimate $p_i$, the probability of success for each participant.

$$
\begin{align*}
y_i &= \operatorname{Binomial}(n_i~\text{trials}, p_i) & \\
p_i &= \operatorname{inv\_logit}(\eta_i) & \\
\eta_i &= b_0 + \alpha_{i} & \\
\alpha_{i} &\sim \operatorname{Normal}(0, \sigma) &\\
i &: \text{participant index} &\\
y &: \text{observed number of successes} &\\
p &: \text{probability of success} &\\
\eta &: \text{linear model on the logit scale}\\
b_0 &: \text{intercept, logit of success for typical participant} &\\
\alpha_i &: \text{participant's adjustment from the typical value} &\\
\alpha_i &: \text{participant's adjustment from the typical value} &\\
\end{align*}
$$



## Terminology

Each participant has their personal probability of correctly choosing word or 
nonword. This probability is the **conditional mean** or 
**subject-specific mean**. The word *conditional* is there to remind that 
there is a conditional probability or expectation at play.

$$
\operatorname{Pr}(\text{correct} \mid \text{participant}_i) \\
\operatorname{E}(\text{correct} \mid \text{participant}_i, n~\text{trials})
$$

But if we remove that conditional part, we have a **marginal mean** or
**population mean** or **population-averaged mean**:

$$
\operatorname{E}{Pr}(\text{correct}) \\
\operatorname{E}(\text{correct} \mid n~\text{trials})
$$

And we can remove that conditional part by averaging  (or marginalizing) over all of the participants. See the [law of total expectation](https://en.wikipedia.org/wiki/Law_of_total_expectation#Informal_proof) with $\operatorname{E}(X) = \operatorname{E}(\operatorname{E}(X \mid Y))$ or the [definition of a marginal probability](https://en.wikipedia.org/wiki/Marginal_distribution). The purpose of this post is demonstrate various ways to achieve this marginalization.






Let's hammer out the terminology right away. There is a population of 
participants, and they each have their own probability of correctly choosing *word* or *nonword*. 


```{r}

tibble(
  x = qnorm(ppoints(100), 2, 1),
  y = plogis(x),
  x_m = mean(x),
  y_m = mean(y),
) |> 
  ggplot() + geom_point(aes(x = x, y = y)) +
  geom_vline(aes(xintercept = x_m)) +
  geom_hline(aes(yintercept = y_m)) +
  geom_density(aes(x = x, y = after_stat(scaled) / 5)) + 
  geom_density(aes(y = y, x = after_stat(scaled) - 1), orientation = "y")
  

logitnorm_mean <- wisclabmisc::logitnorm_mean
means <- ppoints(200)
logits <- qlogis(means)
sd <- .5

p <- expand.grid(means = means, sd = c(.1, .25, .5, .75, 1, 1.5, 2)) |> 
  mutate(
    logits = qlogis(means), 
    y = logitnorm_mean(logits, sd)
  )
ggplot(p) + aes(x = means, y = y) + geom_line(aes(group = sd, color = sd))
tibble(
  x = means,
  y = logitnorm_mean(logits, sd)
) 


# |> 
#   ggplot() + geom_line(aes(x,y)) + geom_abline()


ggplot() + 
  stat_function(
    fun = logitnorm::dlogitnorm, 
    args = list(mu = 0, sigma = 5)
  )

logitnorm::dlogitnorm()


ggplot()
```


## Easy case: logitnorm




```{r}
data_real_speed <- a |> 
  filter(stim_cat == "word", condition == "speed")

model_real_speed <- glmer(
  cbind(n_correct, n_incorrect) ~ 1 + (1 | id),
  family = binomial(),
  data = data_real_speed
)
summary(model_real_speed)
```


The fixed effects here describe a *statistically average* or
*statistically typical* participant. We need to emphasize the word
*statistically* here because this value only makes sense in the context
of this data and this model. The fixed effects predictions here are
conditional, or subject-specific, predictions. They happen to be the
prediction for a participant whose $\alpha_i = 0$.

To convert from a conditional prediction to a marginal one, we can
simulate new participants by drawing them from a normal distribution.

```{r}
inv_logit <- brms::inv_logit_scaled

sigma <- VarCorr(model_real_speed) |> getElement("id") |> attr("stddev")

new_alphas <- rnorm(1000, 0, sigma)


means_real_speed <- list()

means_real_speed$typical <- fixef(model_real_speed) |> 
  inv_logit() |> 
  unname()

means_real_speed$marginal_rnorm <- 
  (fixef(model_real_speed) + new_alphas) |> 
  inv_logit() |> 
  mean()
```

The logitnorm R package can do this calculation using numerical
integration (as opposed to simulating new participants):

```{r}
means_real_speed$marginal_logitnorm <- model_real_speed |> 
  fixef() |> 
  logitnorm::momentsLogitnorm(mu = _, sigma = sigma) |> 
  getElement("mean")
```

In my research for this post, I found the following approximation:

```{r}
var_logistic <- ((pi ^ 2) / 3) * (15 / 16)
b_pa <- fixef(model_real_speed) / sqrt(((sigma ^ 2) + var_logistic) / var_logistic)
means_real_speed$marginal_approx <- unname(inv_logit(b_pa))
```

[Demidenko (2004, 2013)](https://www.eugened.org/mixed-models) also provides this approximation:

```{r}
adjustment <- (sigma ^ 2) / 1.7 ^ 2
adjustment <- 1 / sqrt(1 + adjustment)
b_pa <- fixef(model_real_speed) * adjustment
means_real_speed$marginal_approx_2 <- unname(inv_logit(b_pa))
```





Another kind of mean we might want is the average conditional mean:

```{r}
means_real_speed$avg_conditional <- model_real_speed |> 
  coef() |> 
  getElement("id") |> 
  as.matrix() |> 
  inv_logit() |> 
  mean()
```

```{r}
means_real_speed
```


Let's move on to a random-slope model.


```{r}
data_real <- a |> 
  filter(stim_cat == "word")

model_real <- glmer(
  cbind(n_correct, n_incorrect) ~ condition + (condition | id),
  family = binomial(),
  data = data_real
)
summary(model_real)
```

Here, the random effects are correlated, so to simulate new participants, we 
need to sample from the multivariate normal distribution. logitnorm cannot help 
us at this point.


```{r}
m <- matrix(c(1, 0, 1, 1), nrow = 2)

sd <- VarCorr(model_real) |> getElement("id") |> attr("stddev")
cor <- VarCorr(model_real) |> getElement("id") |> attr("correlation")

sim <- faux::rnorm_multi(
  n = 1000,
  mu = fixef(model_real),
  sd = sd,
  r = cor,
  as.matrix = TRUE
)
means2 <- list()

means2$marginal_rnorm <- inv_logit(sim %*% m) |> 
  colMeans() |> setNames(c("accuracy", "speed"))

# sim[,2] <- sim[,1] + sim[,2]

means2$typical <- inv_logit(fixef(model_real) %*% m) |> 
  as.vector() |> 
  setNames(c("accuracy", "speed"))

```

Again, with the average conditional mean.

```{r}
subj <- coef(model_real) |> getElement("id") |> as.matrix()

means2$avg_conditional <- inv_logit(subj %*% m) |> 
  colMeans() |> 
  setNames(c("accuracy", "speed"))
```


```{r}
means2
```

## Bayesian models

The above examples were manageable and straightforward. But what about in a 
Bayesian model where instead of a single correlation matrix we have a whole 
posterior distribution of them (or rather, 4000 draws from the posterior distribution)?

The rvar (random variable) datatype from the posterior package can help us 
tremendously here because it lets you treat the abstract over draws, in a way. This is hard to explain.

```{r}

library(brms)
f <- bf(
  n_correct | trials(n_trials) ~ condition + (condition | id), 
  family = binomial()
)
p <- c(
  set_prior("normal(0, 2)", class = "Intercept"),
  set_prior("normal(0, 1)", class = "b"),
  set_prior("normal(0, 1)", class = "sd"),
  set_prior("lkj(2)", class = "cor")
)
validate_prior(p, f, data = data_real)


bayes_real <- brm(
  f, 
  data_real, 
  prior = p, 
  backend = "cmdstanr", 
  file = "model_real"
)
```


Let's do the fixed effects conditional means first. By default,
`fixef()` summarize the posterior distribution of the fixed effects for
us.

```{r}
library(posterior)
set_colnames <- `colnames<-`

mean_bayes <- list()

fixef(bayes_real)

bayes_fixef <- bayes_real |> 
  fixef(summary = FALSE) |> 
  rvar(dim = 2)
bayes_fixef

mean_bayes$typical <- inv_logit(bayes_fixef %**% m) |> 
  # matrix multiplications gives 1 x 2 matrix rvar so 
  # reset two be a length-two vector rvar
  rvar(dim = 2) |>  
  setNames(c("accuracy", "speed")) 
mean_bayes
```

`bayes_fixef` is a two-variable rvar on 4000 draws. We would normally
treat it as a 4000 x 2 matrix (draws x variables) but as an rvar, we get
to mostly treat it like the 2-element vector from earlier.

One twist here is that rvar uses `%**%` for matrix multiplication
instead of `%*%`.


Now, for the next step up in difficulty, let's take the average
conditional mean. As in the lme4 case, `coef()` will return
subject-specific parameters and we can matrix-multiply them by the
simple design matrix to get the fitted values for each condition.

```{r}
coef_rvar <- bayes_real |> 
  coef(summary = FALSE) |> 
  getElement("id") |> 
  rvar()

bayes_ss <- inv_logit(coef_rvar %**% m)

colnames(bayes_ss) <- c("accuracy", "speed")
bayes_ss

bayes_ss
```

At this point, we have 4000 x 17 x 2 condition means and we want 4000
x 2 condition means by averaging over the 17 participants in each draw. 
```{r}
mean_bayes$avg_conditional <- bayes_ss |> 
  rvar_apply(.margin = 2, rvar_mean)

mean_bayes


```

The code is completely analogous to what we would write in for a regular matrix.

```{r}
# mean <- matrix |> apply(margin = 2, mean)
```


Finally, we can tell rvar to just do it's thing.

```{r}
# m <- matrix(c(1, 0, 1, 1), nrow = 2)
bayes_fixef

bayes_sd <- bayes_real |> 
  VarCorr(summary = FALSE) |> 
  purrr::pluck("id", "sd") |> 
  rvar()

bayes_cor <- bayes_real |> 
  VarCorr(summary = FALSE) |> 
  purrr::pluck("id", "cor") |> 
  rvar()

sim <- rdo(
  faux::rnorm_multi(
    n = 1000,
    mu = bayes_fixef,
    sd = bayes_sd,
    r = bayes_cor,
    as.matrix = TRUE
  )
)
head(sim)
```


```{r}
mean_bayes$marginal <- inv_logit(sim %**% m) |> rvar_apply(2, rvar_mean)
mean_bayes
```





```{r}
aa <- a |> 
  filter(stim_cat == "word")

aa_data <- aa

m <-
summary(m)
conditional_effects(m)


ggplot(aa_data) + 
  aes(x = condition, y = n_correct / n_trials) +
  geom_line(aes(group = id), stat = "summary")
summary(m)
```



```{r}
d |> 
  filter(condition == "accuracy") |> 
  ggplot() + 
  aes(x = freq, y = correct) + 
  stat_summary(aes(color = stim_cat, group = id:stim_cat), geom = "line", position = position_jitter(width = .1, height = 0))
```


```{r}
a <- d |> 
  filter(!censor) |> 
  group_by(id, condition, stim_cat, frequency, freq) |> 
  summarise(
    n_correct = sum(correct),
    n_trials = n()
  ) |> 
  ungroup()
library(lme4)
p <- glmer(
  cbind(n_correct, n_trials - n_correct) ~ freq  + (freq | id),
  family = binomial(),
  data = a |> filter(condition == "accuracy", stim_cat == "word")
)
summary(p)
```

```{r}
library(brms)
cmdstanr::check_cmdstan_toolchain()

data <- a |> filter(condition == "accuracy", stim_cat == "word") |> 
  mutate(freq = ordered(freq, c("high", "low", "very_low")))
f <- bf(
  n_correct | trials(n_trials) ~ mo(freq) + (mo(freq) | id), 
  family = binomial()
)
f
p <- c(
  set_prior("normal(0, 2)", class = "Intercept"),
  set_prior("normal(0, 1)", class = "b"),
  set_prior("normal(0, 1)", class = "sd"),
  set_prior("lkj(2)", class = "cor")
  
)
validate_prior(p, f, data = data)

m <- brm(f, data, prior = p, backend = "cmdstanr")
summary(m)
conditional_effects(m)
```

Instead, here's the recipe,  using rvar.

```{r}
library(posterior)

m$basis
?mo
fixef(m)
m
new_participant <- data |> 
  distinct(freq) |> 
  mutate(id = "fake", n_trials = 1, n_correct = 0) 


posterior_summary(m, variable = "simo_mofreq1")


posterior_epred(m, new_participant, re_formula = NA) |> rvar()

r <- as_draws_rvars(m, c("b_Intercept", "simo_mofreq1", "bsp_mofreq"))

# intercept (high frequency)
brms::inv_logit_scaled(r$b_Intercept)

# low frequency
brms::inv_logit_scaled(
  r$b_Intercept + 
    2 * r$bsp_mofreq * r$simo_mofreq1[1]
)

# very low frequency
brms::inv_logit_scaled(
  r$b_Intercept + 
    2 * r$bsp_mofreq * r$simo_mofreq1[1] +
    2 * r$bsp_mofreq * r$simo_mofreq1[2]
)
# 2 is the number of cut points (or the number of levels minus 1)



rvar(posterior_epred(
  m, 
  newdata = new_participant, 
  re_formula = NA
))

coef(m, pars = "mofreq1")
str(coef(m))
posterior_epred(m, new_participant, resp = "mofreq1", re_formula = NA)
fixef()
m
coef(m)
VarCorr(m, summary = FALSE, )[["id"]][["cov"]] |> rvar()
rvar()
```



