---
title: Draft post (2025-11-03)
excerpt: ''
tags: ''
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE
)
if (interactive()) setwd("_R/_drafts")
```

```{r, eval = FALSE, include = FALSE}
# Run-once code to produce the spectrogram from the wav file
spectrogram_out <- "../data/mfa/library-tidyverse-library-brms.Spectrogram"

f_create_spectrogram <- tjm.praat::wrap_praat_script(
  script_code_to_run = tjm.praat::create_spectrogram,
  returning = "last-argument"
)

data_spectrogram <- "../data/mfa/library-tidyverse-library-brms.wav" |> 
  f_create_spectrogram(
    spectrogram_out = spectrogram_out, 
    max_frequency = 6000
  ) 

data_spectrogram |> 
  tjm.praat::read_spectrogram() |> 
  readr::write_csv("../data/mfa/library-tidyverse-library-brms.csv")
```

In this post, I announce the release of readtextgrid 0.2.0 and describe
some experience with AI-assisted coding that occurred along the way.

***

The following image is a screenshot of a "textgrid" the acoustic analysis
program Praat.


{% include figure image_path="/assets/images/2025-11-library-tidyverse.png" alt="Screenshot of a Praat editor window showing the amplitude wave form, spectrogram, and textgrid annotations." caption="Figure 1. Screenshot of a Praat editor window." %}{: style="max-width: 100%; display: block; margin: 2em auto;"}


There are three main subwindows in the image:

1.  Amplitude wave form
2.  Spectrogram, showing how the intensity (*color*) at frequencies (*y*) changes over
    time (*x*)
3.  Textgrid of text annotations for the recording

This textgrid is the result of forced alignment, specifically by the
[Montreal Forced Aligner][mfa]. I told the program I said "library tidy verse
library b r m s", and it looked up the pronunciations of those words and
used an acoustic model to lay down the time intervals of each word and
each speech sound. The aligner produced a .TextGrid file, a file format
used by Praat to store this kind of temporal annotation.

[mfa]: https://montreal-forced-aligner.readthedocs.io/en/latest/ "Montreal Forced Aligner homepage"

These textgrids are the bread and butter of some of the research that we
do. For example, [our article][wf2] on speaking/articulation rate in children
involved over 30,000 single-sentence .wav files and .TextGrid files. We
used the alignments to determine the duration of time spent speaking, the
number of vowels in each utterance and hence the speaking rate in
syllables per second.

[wf2]: https://pubs.asha.org/doi/10.1044/2021_JSLHR-21-00206 "Speech Development Between 30 and 119 Months in Typical Children II: Articulation Rate Growth Curves"

Getting these textgrid files into R was cumbersome, so I wrote and
released [readtextgrid][rtg-cran], an R package with one simple function:

[rtg-cran]: https://cran.r-project.org/package=readtextgrid "readtextgrid on CRAN"

```{r}
library(readtextgrid)

path_tg <- "../data/mfa-out/library-tidyverse-library-brms.TextGrid" 
data_tg <- read_textgrid(path_tg)

data_tg
```

This dataframe only
design is opionionated. There is no route from a dataframe back to a .TextGrid
fi.e.

The function returns a tidy tibble with one row per annotation. The filename is
stored as a column name so that we can `lapply()` over a directory of files.
Annotations are numbered so that we can `group_by(text, annotation_num)` so
that repeated words, e.g., can be handled separately. 


With this textgrid in R, I could measure speaking rate, for example:

```{r}
data_tg |> 
  filter(tier_name == "phones", text != "") |> 
  summarise(
    speaking_time = sum(xmax - xmin),
    # vowels have numbers to indicate degree of stress
    num_vowels = sum(str_detect(text, "\\d"))
  ) |> 
  mutate(
    syllables_per_sec = num_vowels / speaking_time 
  )
```

Or annotate a spectrogram:

```{r}
library(tidyverse)
library(ggplot2)
path_spectrogram <- "../data/mfa/library-tidyverse-library-brms.csv"
data_spectrogram <- readr::read_csv(path_spectrogram)

data_spectrogram |> 
  mutate(
    # save more of the color variation for intensities above 15 dbs
    db = ifelse(db < 15, 15, db)
  ) |> 
  ggplot() + 
  aes(x = time, y = frequency) +
  geom_raster(aes(fill = db)) +
  geom_text(
    aes(label = text, x = (xmin + xmax) / 2),
    data = data_tg |> filter(tier_name == "words"),
    y = 6500,
    vjust = 0
  )  +
  geom_text(
    aes(label = text, x = (xmin + xmax) / 2),
    data = data_tg |> filter(tier_name == "phones"),
    y = 6100,
    vjust = 0,
    size = 2
  )  +
  ylim(c(NA, 6600)) +
  theme_minimal() +
  scale_fill_gradient(low = "white", high = "black") +
  labs(x = "time [s]", y = "frequency [Hz]", fill = "dB SPL")
```










I released the first version of the package in 2021. This package,
notably for me, contains the first hex badge I ever made.

<!-- hex badge here -->

<!-- I think it did a satisfactory job. I can see that the /S/ intervals have high frequency noise, that the second interval of "m" has a sudden dampening of energy (due to nasal airflow of /m/) -->


## The original parser and the problem

Here is what the contents of the .TextGrid file look like. It's not the whole
file but enought to give a sense of the structure:

```{r}
path_tg |> 
  readLines() |> 
  head(26) |> 
  c("[... TRUNCATED ... ]") |> 
  writeLines()
```

The first 7 lines provide some metadata about the time range of the
audio and the number of tiers (`size = 2`). It then writes outs each
tier (`item [n]` lines) by first giving the `class`, `name`, time
duration and number of marks or intervals. Each mark or interval is
enumerated with time values `xmin`, `xmax` and `text` values.

Because nearly everything here follows a `key = value` syntax and
because sections are split from each other very neatly with `item [n]:`
or `interval [n]:` lines, I was able to write **a simple parser using
regular expressions**: Split the file into `item [n]` sections, split
those into `interval [n]` sections, and extract key-value pairs.

But this easy approach came with limitations. First, the TextGrid
specification was much more flexible. For example, Praat also provides
much less verbose "short" format textgrids. The file in our case would
just be a stream of time and text annotations:

```{r}
path_tg_short <- "../data/mfa-out/library-tidyverse-library-brms-short.TextGrid"
path_tg_short |> 
  readLines() |> 
  head(26) |> 
  c("[... TRUNCATED ... ]") |> 
  writeLines()
```

Everything is in the
same order, but the annotations are gone. All of the helpful labels from
above are actually *comments* that get ignored. Everything that isn't a 
number or a string in double-quotes (or a `<flag>`) is a comment.

There are also other quirks (`"` escapement, `!` comments, deviations between
the Praat description of the format and the behavior of `praat.exe`). I have
them documented as a kind of [unofficial specification][unofficial-spec] as an article on
the package website.

[unofficial-spec]: https://www.tjmahr.com/readtextgrid/articles/textgrid-specification.html "Textgrid Specification article"

But my original regular-expression based parser could only handle the verbose
long-format textgrids. I knew this. I put this [a GitHub issue in 2020][rtg-issue-4]. And this
compatibility oversight was never a problem for me until I tried a new phonetics
tool that defaulted to saving the textgrids in the short format. Now
readtextgrid could not handle the problem.

[rtg-issue-4]: https://github.com/tjmahr/readtextgrid/issues/4 "ugh support textgrid grammar #4"

## The R-based tokenizer

[Josef Fruehwald][jf-1], a linguist with [lots of acoustics/phonetics software][jf-2], submitted a pull request to do a proper parser that I eventually rewrote to handle various edge cases and undocumented behavior in the .TextGrid specification. I made an [adversarial .TextGrid file][hard-file] ðŸ˜ˆ that could still be opened by `praat.exe` but was meant to be difficult to parse.
This was a fun development loop: Make the file harder, update the parser to handle the new feature, repeat.

[hard-file]: https://github.com/tjmahr/readtextgrid/blob/ed971e48ab3ea33e3efe0ba59f45ae3e41d07a32/tests/testthat/test-data/hard-to-parse.TextGrid "hard-to-parse.TextGrid on GitHub"
[jf-1]: https://jofrhwld.github.io/ "Josef Fruehwald's homepage"
[jf-2]: https://jofrhwld.github.io/software/ "Josef Fruehwald's software page"

Because the essential data in the file data are just string tokens and number
tokens, I needed to make a [tokenizer][wiki-lexer]: A piece of software that reads
in characters, groups them into tokens, and figures out what kind of data the
token represents. The initial R-based version of the tokenizer did the following: 

  - Read the file character by character
  - Gather the characters for the current token and keep them when they
    form a valid string or number.
  - Shift between three states (`in_string`, `in_strong_comment` (`!
    comments`), `in_escaped_quote`). 

[wiki-lexer]: https://en.wikipedia.org/wiki/Lexical_analysis "Wikipedia page on Lexical Analysis"

These three states determine how we interpret spaces, newlines, and `"`
characters. For example, a newline ends a `! comment` but a newline can appear
in a string so it doesn't end a string. Moreover, in a comment, `"` is ignored, but in a
string, it might be the end of the string or an escaped string (doubled
double-quotes are `"` characters: the string`"""a"""` has the text `"a"`.

The new R character-by-character-based parser worked ðŸŽ‰. It had conquered
the adversarial example file. But there was another problem. It was slower than
the original regular-expression-based parser.

```{r}
path_tg_lines <- readLines(path_tg)

bench::mark(
  legacy = readtextgrid:::legacy_read_textgrid_lines(path_tg_lines),
  new_r_parser = readtextgrid:::r_read_textgrid_lines(path_tg_lines)
)
```


## Pivot to LLMs












```{asis}
<blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:4t2ziwnnescprzorvmrfduey/app.bsky.feed.post/3ltdjbaktss2s" data-bluesky-cid="bafyreihipumdkez3ifgsyt3eqbgelzuzljvllfwj4lbnfju55vvoxqc5gu" data-bluesky-embed-color-mode="system"><p lang="en">the thing about (the current) chatgpt is that it writes like a fucking idiot with excellent grammar</p>&mdash; sarah jeong (<a href="https://bsky.app/profile/did:plc:4t2ziwnnescprzorvmrfduey?ref_src=embed">@sarahjeong.bsky.social</a>) <a href="https://bsky.app/profile/did:plc:4t2ziwnnescprzorvmrfduey/post/3ltdjbaktss2s?ref_src=embed">July 6, 2025 at 7:20 PM</a></blockquote>
```




Now, let's talk about LLMs. There's a lot I could say about them. As a language
scientist, I'll start here: They know *syntax*. They know which words go
together and can generate very plausible sequences of words. They do not know
*semantics* however. They don't have any firsthand knowledge or experience about what
they say. They can't introspect about that knowledge or experience to see
whether things "make sense". They [don't
care](https://www.andrewheiss.com/ai/#text) about the truth or falsity of
statements. They just make plausible sequences of words. 

Now, it turns out that if you learn how to make sequences of words from an
Internet-sized corpus of text, then a lot of the plausible sequences you make
will turn out to be true. If you read 10,000 cookbooks, you could probably
provide a very classic recipe for scrambled eggs. But because you don't know
about sarcasm or can't draw on your own experience of trying to not ingest
chemicals, you might suggest putting glue on a pizza. {}

So, as we use an LLM, we need to ask ourselves how much we care about the truth
or care about knowing or understanding things. That may sound like a glib or
weird statement: Shouldn't we always care about the truth? Well, sometimes we
don't. We just want some syntax; we want boilerplate to fill out. For example, I ask "write some unit
tests for a function `round_to(xs, unit)` that rounds a vector of values to an
arbitrary unit" and receive:

```{r, eval = FALSE}
test_that("round_to() rounds to nearest multiple of unit", {
  expect_equal(round_to(5, 2), 6)
  expect_equal(round_to(4.9, 2), 4)
  expect_equal(round_to(5.1, 2), 6)
  expect_equal(round_to(c(1, 2, 3, 4), 2), c(2, 2, 4, 4))
})
```

These tests are not useful *until* I plug in the correct values for the expected 
output.

In other cases, we don't quite care about truth or comprehension because we can
get external corroboration. When I ask ChatGPT for an obfuscated R script to
make Pac Man in ggplot2, I can run the code to see if it works without trying to
decipher its syntax:

```{r, error=TRUE}
library(ggplot2)
ggplot()+
geom_polygon(aes(x,y),
data=within(data.frame(t(sapply(seq(a<-pi/9,2*pi-a,l<-4e2),
function(t)c(cos(t),sin(t))))),
{rbind(.,0,0,cos(a),sin(a))->df;x=df[,1];y=df[,2]}),
fill="#FF0",col=1)+
annotate("point",x=.35,y=.5,size=3)+
annotate("point",x=c(1.4,2,2.6),y=0,size=3)+
coord_equal(xlim=c(-1.2,3),ylim=c(-1.2,1.2))+
theme_void()
```

[Well that sucks.](https://www.youtube.com/watch?v=NxSj2T2vx7M)

When we abandon caring about truth or understanding things, and just rely on
external corroboration, we are in the realm of "vibe" coding. I like this term
because of its insouciant honesty: *Truth? Comprehension? We're just going off
the vibes.* It would be a great help if we used the word more liberally. A
Youtube video called "A vibe history of NES videogames"? No thanks.


The two principles I am trying to develop in this mini-position statement is that LLM assistance are

- LLMs know text distributions very well. Use them to generate syntax. 
- LLMs don't understand anything. It's all bullshit and vibes. 


```{asis}
<blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:v6qwaqo24zfrq5fj7ceibxqk/app.bsky.feed.post/3lp42tel3lk2e" data-bluesky-cid="bafyreigsxov4rjblnirmd7samors5k6ygw63lfrenx35g5iwczrs6eawxi" data-bluesky-embed-color-mode="system"><p lang="en">re AI, a PhD student mentioned sheepishly that they used chatgpt for advice on coding up an unusual element in javascript. Almost apologized

I&#x27;m like no no no you&#x27;re a psych PhD, not CS, this is exactly what LLMs are for! Doing a so-so job at things you just need done &amp; don&#x27;t care about learning!</p>&mdash; samuel mehr (<a href="https://bsky.app/profile/did:plc:v6qwaqo24zfrq5fj7ceibxqk?ref_src=embed">@mehr.nz</a>) <a href="https://bsky.app/profile/did:plc:v6qwaqo24zfrq5fj7ceibxqk/post/3lp42tel3lk2e?ref_src=embed">May 13, 2025 at 10:32 PM</a></blockquote>
```


## AI in this case


I asked ChatGPT for ways to make it faster. For example, one version of the loop
collected characters in a vector and then `paste0()`-ed them together. ChatGPT
suggested instead because we are iterating over character indices, use
`substring()` to extract tokens from the text. That worked, and was faster,
until it failed a unit test on a character wearing diacritic on it. After a few
rounds of trying to improve the loop, I asked quite bluntly: "How can we move
the tokenize loop into Rcpp or cpp11 with the viewest [*sic*] headaches
possible".

What I liked here is how clear the translation was. In R, part of the loop is to
peek ahead to the next character to see it `"` is an escaped quote `""` or the
end of a string. Here is the R version:

```{r}
# ... in the character processing loop

    # Start or close string mode if we see "
    if (c_starts_string) {
      # Check for "" escapes
      peek_c <- all_char[i + 1]
      if (peek_c == "\"" & in_string) {
        in_escaped_quote <- TRUE
      } else {
        in_string <- !in_string
      }
    }

# ...
```

And here is the C++ version:

```{c++, eval = FALSE}
// ... helper functions

  // Is this a UTF-8 continuation byte? (10xxxxxx)
  auto is_cont = [](unsigned char b)->bool {
    // Are the first two bits 10?
    return (b & 0xC0) == 0x80;
  };

// ... in the character processing loop ...

    if (b == 0x22) { // '"'
      // peek ahead to see if we have a double "" escapement
      size_t j = i + 1;
      // We need the next character, not just the next byte, so we skip
      // continuation characters.
      while (j < nbytes && is_cont(static_cast<unsigned char>(src[j]))) ++j;
      // Use `0x00` dummy character if we are at then end of the string
      unsigned char nextb = (j < nbytes) ? static_cast<unsigned char>(src[j]) : 0x00;

      if (in_string && nextb == 0x22) {
        esc_next = true;    // consume next '"' once
      } else {
        in_string = !in_string;
      }
    }

// ...
```

There is a logical correspondence between the lines that I wrote myself in R 
and the lines that the LLM provided for C++. Yes, the 
C++ version works at the level of bytes instead of characters.

```{r}
"Ã©" |> nchar(type = "chars")
"Ã©" |> nchar(type = "bytes")
```

But the C++ code make sense to me. It looks *plausible*, right? Still, I
asked the LMM a ton of questions: what does `auto` do, what is `size_t`
doing\`, and so on.

This translation strikes me as an easy win for R developers right now:
"I have my version that works right now, but I think it can go faster.
Help me convert this to C++."

***

In Praat, numbers use `.` for the decimal point. It's quite un-French.












[^abadox]: I am still immensely annoyed about a YouTube video that tried to 
  tell me Abadox was a "controversial" NES game. Get out of here. Nobody 
  talked about that game. Go ahead. Show me a newspaper clipping or something.
  




[^death]: When you don't know the value of life or death, you can encourage people to do dangerous things. 






I asked ChatGPT for help 
making a shopping list for a small woodworking project and it offered a cutting plan for the lumber. It messed up the math with a plan that involved cutting off 74 inches of wood from a 6-foot piece of lumber.


So, an LLM 


When we use an LLM, we have to ask ourselves how much we care about making sense. 


But if you memorize a 
million cookbooks, you'll be able to generate a lot of recipes.










I hate getting
a chat or an email that sounds like AI. I hate hearing a YouTube video about 
music or videogame history that just straight up fabricates



```{r, include = FALSE}
.parent_doc <- knitr::current_input()
```
```{r, change_to_child_when_done = "_footer.Rmd"}
```
